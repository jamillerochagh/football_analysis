{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Home      Away  Round        Date   Time   WIN  minuto       evento  \\\n",
      "0  Birmingham  West Ham     38  11.05.2003  16:00  Draw      27  Yellow_Away   \n",
      "1  Birmingham  West Ham     38  11.05.2003  16:00  Draw      57  Yellow_Home   \n",
      "2  Birmingham  West Ham     38  11.05.2003  16:00  Draw      66    Goal_Away   \n",
      "3  Birmingham  West Ham     38  11.05.2003  16:00  Draw      80    Goal_Home   \n",
      "4  Birmingham  West Ham     38  11.05.2003  16:00  Draw      88    Goal_Home   \n",
      "\n",
      "   Yellow_Away  Yellow_Home  Goal_Away  Goal_Home  Red_Card_Away  \\\n",
      "0            1            0          0          0              0   \n",
      "1            1            1          0          0              0   \n",
      "2            1            1          1          0              0   \n",
      "3            1            1          1          1              0   \n",
      "4            1            1          1          2              0   \n",
      "\n",
      "   Red_Card_Home  Own_Home  Own_Away  Penalty_Missed_Home  \\\n",
      "0              0         0         0                    0   \n",
      "1              0         0         0                    0   \n",
      "2              0         0         0                    0   \n",
      "3              0         0         0                    0   \n",
      "4              0         0         0                    0   \n",
      "\n",
      "   Penalty_Missed_Away  Var_Home  Var_Away  \n",
      "0                    0         0         0  \n",
      "1                    0         0         0  \n",
      "2                    0         0         0  \n",
      "3                    0         0         0  \n",
      "4                    0         0         0  \n"
     ]
    }
   ],
   "source": [
    "# Loading datasets\n",
    "train_df = pd.read_csv('/Users/jamille.ghazaleh/Downloads/CEUB/1-avaliacao-pratica-2-2024-ml-uniceub/train_futebol.csv')\n",
    "test_df = pd.read_csv('/Users/jamille.ghazaleh/Downloads/CEUB/1-avaliacao-pratica-2-2024-ml-uniceub/test_futebol.csv')\n",
    "submission_df = pd.read_csv('/Users/jamille.ghazaleh/Downloads/CEUB/1-avaliacao-pratica-2-2024-ml-uniceub/sample_submission.csv')\n",
    "\n",
    "print(train_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40330, 20)\n"
     ]
    }
   ],
   "source": [
    "# Understanding dimensions\n",
    "print(train_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              Round        minuto   Yellow_Away   Yellow_Home     Goal_Away  \\\n",
      "count  40330.000000  40330.000000  40330.000000  40330.000000  40330.000000   \n",
      "mean      18.978031     54.656534      1.157575      0.926878      0.799207   \n",
      "std       10.954710     25.333753      1.136995      1.026580      0.938274   \n",
      "min        1.000000      0.000000      0.000000      0.000000      0.000000   \n",
      "25%        9.000000     35.000000      0.000000      0.000000      0.000000   \n",
      "50%       19.000000     57.000000      1.000000      1.000000      1.000000   \n",
      "75%       28.000000     77.000000      2.000000      1.000000      1.000000   \n",
      "max       38.000000    101.000000      9.000000      7.000000      9.000000   \n",
      "\n",
      "          Goal_Home  Red_Card_Away  Red_Card_Home      Own_Home      Own_Away  \\\n",
      "count  40330.000000   40330.000000   40330.000000  40330.000000  40330.000000   \n",
      "mean       1.026382       0.054947       0.038978      0.040863      0.029259   \n",
      "std        1.063413       0.235162       0.200221      0.204748      0.171739   \n",
      "min        0.000000       0.000000       0.000000      0.000000      0.000000   \n",
      "25%        0.000000       0.000000       0.000000      0.000000      0.000000   \n",
      "50%        1.000000       0.000000       0.000000      0.000000      0.000000   \n",
      "75%        2.000000       0.000000       0.000000      0.000000      0.000000   \n",
      "max        9.000000       2.000000       3.000000      3.000000      2.000000   \n",
      "\n",
      "       Penalty_Missed_Home  Penalty_Missed_Away      Var_Home      Var_Away  \n",
      "count         40330.000000         40330.000000  40330.000000  40330.000000  \n",
      "mean              0.009943             0.005653      0.002628      0.001587  \n",
      "std               0.102416             0.076936      0.051200      0.042806  \n",
      "min               0.000000             0.000000      0.000000      0.000000  \n",
      "25%               0.000000             0.000000      0.000000      0.000000  \n",
      "50%               0.000000             0.000000      0.000000      0.000000  \n",
      "75%               0.000000             0.000000      0.000000      0.000000  \n",
      "max               2.000000             2.000000      1.000000      2.000000  \n"
     ]
    }
   ],
   "source": [
    "# Statistics\n",
    "print(train_df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Home                   0\n",
      "Away                   0\n",
      "Round                  0\n",
      "Date                   0\n",
      "Time                   0\n",
      "WIN                    0\n",
      "minuto                 0\n",
      "evento                 0\n",
      "Yellow_Away            0\n",
      "Yellow_Home            0\n",
      "Goal_Away              0\n",
      "Goal_Home              0\n",
      "Red_Card_Away          0\n",
      "Red_Card_Home          0\n",
      "Own_Home               0\n",
      "Own_Away               0\n",
      "Penalty_Missed_Home    0\n",
      "Penalty_Missed_Away    0\n",
      "Var_Home               0\n",
      "Var_Away               0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Data cleaning\n",
    "print(train_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Home                   object\n",
      "Away                   object\n",
      "Round                   int64\n",
      "Date                   object\n",
      "Time                   object\n",
      "WIN                    object\n",
      "minuto                  int64\n",
      "evento                 object\n",
      "Yellow_Away             int64\n",
      "Yellow_Home             int64\n",
      "Goal_Away               int64\n",
      "Goal_Home               int64\n",
      "Red_Card_Away           int64\n",
      "Red_Card_Home           int64\n",
      "Own_Home                int64\n",
      "Own_Away                int64\n",
      "Penalty_Missed_Home     int64\n",
      "Penalty_Missed_Away     int64\n",
      "Var_Home                int64\n",
      "Var_Away                int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Understanding type of data \n",
    "print(train_df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting numeric columns\n",
    "numeric_columns = ['minuto', 'Yellow_Away', 'Yellow_Home', 'Goal_Away', 'Goal_Home', \n",
    "                   'Red_Card_Away', 'Red_Card_Home', 'Own_Home', 'Own_Away', \n",
    "                   'Penalty_Missed_Home', 'Penalty_Missed_Away', 'Var_Home', 'Var_Away']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     minuto  Yellow_Away  Yellow_Home  Goal_Away  Goal_Home  Red_Card_Away  \\\n",
      "0  0.267327     0.111111     0.000000   0.000000   0.000000            0.0   \n",
      "1  0.564356     0.111111     0.142857   0.000000   0.000000            0.0   \n",
      "2  0.653465     0.111111     0.142857   0.111111   0.000000            0.0   \n",
      "3  0.792079     0.111111     0.142857   0.111111   0.111111            0.0   \n",
      "4  0.871287     0.111111     0.142857   0.111111   0.222222            0.0   \n",
      "\n",
      "   Red_Card_Home  Own_Home  Own_Away  Penalty_Missed_Home  \\\n",
      "0            0.0       0.0       0.0                  0.0   \n",
      "1            0.0       0.0       0.0                  0.0   \n",
      "2            0.0       0.0       0.0                  0.0   \n",
      "3            0.0       0.0       0.0                  0.0   \n",
      "4            0.0       0.0       0.0                  0.0   \n",
      "\n",
      "   Penalty_Missed_Away  Var_Home  Var_Away  \n",
      "0                  0.0       0.0       0.0  \n",
      "1                  0.0       0.0       0.0  \n",
      "2                  0.0       0.0       0.0  \n",
      "3                  0.0       0.0       0.0  \n",
      "4                  0.0       0.0       0.0  \n"
     ]
    }
   ],
   "source": [
    "# Import the MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Apply the scaler to the numerical columns\n",
    "train_df[numeric_columns] = scaler.fit_transform(train_df[numeric_columns])\n",
    "test_df[numeric_columns] = scaler.transform(test_df[numeric_columns])\n",
    "\n",
    "print(train_df[numeric_columns].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the target variable 'Result'\n",
    "train_df['Result'] = train_df.apply(lambda row: 1 if row['Goal_Home'] > row['Goal_Away'] else (-1 if row['Goal_Home'] < row['Goal_Away'] else 0), axis=1)\n",
    "\n",
    "# Defining the features (X) and target (y)\n",
    "X = train_df.drop(columns=['Result'])\n",
    "y = train_df['Result']\n",
    "\n",
    "# Splitting the data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove columns Date and Time \n",
    "X_train = X_train.drop(columns=['Date', 'Time'], errors='ignore')\n",
    "X_val = X_val.drop(columns=['Date', 'Time'], errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Home                    object\n",
      "Away                    object\n",
      "Round                  float64\n",
      "WIN                     object\n",
      "minuto                 float64\n",
      "evento                  object\n",
      "Yellow_Away            float64\n",
      "Yellow_Home            float64\n",
      "Goal_Away              float64\n",
      "Goal_Home              float64\n",
      "Red_Card_Away          float64\n",
      "Red_Card_Home          float64\n",
      "Own_Home               float64\n",
      "Own_Away               float64\n",
      "Penalty_Missed_Home    float64\n",
      "Penalty_Missed_Away    float64\n",
      "Var_Home               float64\n",
      "Var_Away               float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Scaling the numerical columns\n",
    "scaler = StandardScaler()\n",
    "numeric_columns = X_train.select_dtypes(include=['float64', 'int64']).columns\n",
    "\n",
    "# Applying the scaler to the training data\n",
    "X_train[numeric_columns] = scaler.fit_transform(X_train[numeric_columns])\n",
    "X_val[numeric_columns] = scaler.transform(X_val[numeric_columns])\n",
    "\n",
    "print(X_train.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for the presence of categorical or boolean columns\n",
    "categorical_columns = X_train.select_dtypes(include=['object', 'bool']).columns\n",
    "if len(categorical_columns) > 0:\n",
    "    X_train = pd.get_dummies(X_train, columns=categorical_columns)\n",
    "    X_val = pd.get_dummies(X_val, columns=categorical_columns)\n",
    "\n",
    "# Ensure that the columns match between the training and validation datasets\n",
    "X_val = X_val.reindex(columns=X_train.columns, fill_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round                   float64\n",
      "minuto                  float64\n",
      "Yellow_Away             float64\n",
      "Yellow_Home             float64\n",
      "Goal_Away               float64\n",
      "                         ...   \n",
      "evento_Red_Card_Home       bool\n",
      "evento_Var_Away            bool\n",
      "evento_Var_Home            bool\n",
      "evento_Yellow_Away         bool\n",
      "evento_Yellow_Home         bool\n",
      "Length: 109, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Map the values of 'Result' to numbers\n",
    "y_train = y_train.map({1: 1, 0: 0, -1: -1})\n",
    "y_val = y_val.map({1: 1, 0: 0, -1: -1})\n",
    "\n",
    "# Check if all columns in X_train and X_val are numeric\n",
    "print(X_train.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.9884701214976445\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.99      0.99      0.99      2332\n",
      "           0       0.98      0.98      0.98      2467\n",
      "           1       0.99      0.99      0.99      3267\n",
      "\n",
      "    accuracy                           0.99      8066\n",
      "   macro avg       0.99      0.99      0.99      8066\n",
      "weighted avg       0.99      0.99      0.99      8066\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize the RandomForestClassifier with a fixed random state for reproducibility.\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluation on the validation set\n",
    "y_val_pred = model.predict(X_val)\n",
    "val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "print(f'Validation Accuracy: {val_accuracy}')\n",
    "print(classification_report(y_val, y_val_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the 'WIN' column exists in test_df and map it to numeric values\n",
    "if 'WIN' in test_df.columns:\n",
    "    test_df['WIN'] = test_df['WIN'].map({'Win': 3, 'Lose': 0, 'Draw': 1})\n",
    "\n",
    "# Define the categorical columns expected to be in the test DataFrame\n",
    "categorical_columns_test = ['Home', 'Away', 'WIN', 'evento']\n",
    "\n",
    "# Identify which of these columns actually exist in test_df\n",
    "existing_categorical_columns = [col for col in categorical_columns_test if col in test_df.columns]\n",
    "\n",
    "# Apply pd.get_dummies only to the categorical columns that actually exist in test_df\n",
    "if len(existing_categorical_columns) > 0:\n",
    "    test_df = pd.get_dummies(test_df, columns=existing_categorical_columns)\n",
    "\n",
    "# Ensure that the columns in X_test match those in X_train\n",
    "X_test = test_df.reindex(columns=X_train.columns, fill_value=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test set\n",
    "test_predictions = model.predict(X_test)\n",
    "\n",
    "# Save the predictions for submission\n",
    "submission_df['Result'] = test_predictions\n",
    "submission_df.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Check the accuracy on the training set\n",
    "y_train_pred = model.predict(X_train)\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "print(f'Training Accuracy: {train_accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result\n",
      " 1    16517\n",
      " 0    12416\n",
      "-1    11397\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Print the count of each unique value in the target variable 'y'\n",
    "print(y.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure that all categorical columns are converted to dummy variables\n",
    "X_train = pd.get_dummies(X_train)\n",
    "X_val = pd.get_dummies(X_val)\n",
    "\n",
    "# Ensure that the columns in X_val match those in X_train\n",
    "X_val = X_val.reindex(columns=X_train.columns, fill_value=0)\n",
    "\n",
    "# Check for any non-numeric columns\n",
    "non_numeric_columns = X_train.select_dtypes(include=['object']).columns\n",
    "if len(non_numeric_columns) > 0:\n",
    "    print(f\"Non-numeric columns found: {non_numeric_columns}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply pd.get_dummies to X_test\n",
    "X_test = pd.get_dummies(test_df)\n",
    "\n",
    "# Ensure that the columns in X_test match those in X_train\n",
    "X_test = X_test.reindex(columns=X_train.columns, fill_value=0)\n",
    "\n",
    "# Check for any non-numeric columns in X_test\n",
    "non_numeric_columns_test = X_test.select_dtypes(include=['object']).columns\n",
    "if len(non_numeric_columns_test) > 0:\n",
    "    print(f\"Non-numeric columns found in X_test: {non_numeric_columns_test}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Accuracy with Cross-Validation: 0.9870754328426952\n"
     ]
    }
   ],
   "source": [
    "# Perform cross-validation on the model using the training data\n",
    "scores = cross_val_score(model, X_train, y_train, cv=5)\n",
    "print(f'Average Accuracy with Cross-Validation: {scores.mean()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DecisionTreeClassifier' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Initialize a DecisionTreeClassifier with a maximum depth of 3\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mDecisionTreeClassifier\u001b[49m(max_depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m      3\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Make predictions on the validation set\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'DecisionTreeClassifier' is not defined"
     ]
    }
   ],
   "source": [
    "# Initialize a DecisionTreeClassifier with a maximum depth of 3\n",
    "model = DecisionTreeClassifier(max_depth=3)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the validation set\n",
    "y_pred = model.predict(X_val)\n",
    "accuracy = accuracy_score(y_val, y_pred)\n",
    "print(f'Accuracy with Decision Tree: {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refining Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       1.00      0.71      0.83      2332\n",
      "           0       0.80      0.90      0.84      2467\n",
      "           1       0.90      1.00      0.95      3267\n",
      "\n",
      "    accuracy                           0.89      8066\n",
      "   macro avg       0.90      0.87      0.88      8066\n",
      "weighted avg       0.90      0.89      0.88      8066\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the classification report for the validation set predictions\n",
    "print(classification_report(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Accuracy with Cross-Validation: 0.8928835802472337\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(model, X_train, y_train, cv=5)\n",
    "print(f'Average Accuracy with Cross-Validation: {scores.mean()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique matches after removing duplicates: 931\n",
      "Total rows in the original dataset: 5902\n"
     ]
    }
   ],
   "source": [
    "# Load the test data and the submission file containing predictions\n",
    "test_df = pd.read_csv('/Users/jamille.ghazaleh/Downloads/CEUB/1-avaliacao-pratica-2-2024-ml-uniceub/test_futebol.csv')\n",
    "submission_df = pd.read_csv('submission.csv')\n",
    "\n",
    "# Add the predictions to the test DataFrame\n",
    "test_df['Predicted_Result'] = submission_df['Result']\n",
    "\n",
    "# Check for duplicates after grouping by match identifiers\n",
    "unique_matches_check = test_df.drop_duplicates(subset=['Round', 'Home', 'Away'])\n",
    "\n",
    "print(f\"Total unique matches after removing duplicates: {unique_matches_check.shape[0]}\")\n",
    "print(f\"Total rows in the original dataset: {test_df.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"# Group by 'Round', 'Home', 'Away' to ensure each match is counted once\\nunique_matches = test_df.groupby(['Round', 'Home', 'Away']).agg({'Predicted_Result': 'first'}).reset_index()\\n\\n# Initialize a dictionary to store points for each team\\nteam_points = {}\\n\\n# Iterate through each unique match and assign points\\nfor index, row in unique_matches.iterrows():\\n    home_team = row['Home']\\n    away_team = row['Away']\\n    result = row['Predicted_Result']\\n    \\n    # Initialize the points for the teams if they are not yet in the dictionary\\n    if home_team not in team_points:\\n        team_points[home_team] = 0\\n    if away_team not in team_points:\\n        team_points[away_team] = 0\\n    \\n    # Assign points based on the predicted result\\n    if result == 1:\\n        # Home team wins\\n        team_points[home_team] += 3\\n    elif result == -1:\\n        # Away team wins\\n        team_points[away_team] += 3\\n    else:\\n        # Draw\\n        team_points[home_team] += 1\\n        team_points[away_team] += 1\""
      ]
     },
     "execution_count": 449,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"# Group by 'Round', 'Home', 'Away' to ensure each match is counted once\n",
    "unique_matches = test_df.groupby(['Round', 'Home', 'Away']).agg({'Predicted_Result': 'first'}).reset_index()\n",
    "\n",
    "# Initialize a dictionary to store points for each team\n",
    "team_points = {}\n",
    "\n",
    "# Iterate through each unique match and assign points\n",
    "for index, row in unique_matches.iterrows():\n",
    "    home_team = row['Home']\n",
    "    away_team = row['Away']\n",
    "    result = row['Predicted_Result']\n",
    "    \n",
    "    # Initialize the points for the teams if they are not yet in the dictionary\n",
    "    if home_team not in team_points:\n",
    "        team_points[home_team] = 0\n",
    "    if away_team not in team_points:\n",
    "        team_points[away_team] = 0\n",
    "    \n",
    "    # Assign points based on the predicted result\n",
    "    if result == 1:\n",
    "        # Home team wins\n",
    "        team_points[home_team] += 3\n",
    "    elif result == -1:\n",
    "        # Away team wins\n",
    "        team_points[away_team] += 3\n",
    "    else:\n",
    "        # Draw\n",
    "        team_points[home_team] += 1\n",
    "        team_points[away_team] += 1\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by 'Round', 'Home', 'Away' to ensure each match is counted once\n",
    "# Take the first occurrence of goals for home and away teams to avoid double counting\n",
    "unique_matches = test_df.groupby(['Round', 'Home', 'Away']).agg({\n",
    "    'Predicted_Result': 'first',\n",
    "    'Goal_Home': 'max',\n",
    "    'Goal_Away': 'max'\n",
    "}).reset_index()\n",
    "\n",
    "# Initialize a dictionary to store points, goal difference, and goals scored for each team\n",
    "team_stats = {}\n",
    "\n",
    "# Iterate through each unique match and update team statistics\n",
    "for index, row in unique_matches.iterrows():\n",
    "    home_team = row['Home']\n",
    "    away_team = row['Away']\n",
    "    result = row['Predicted_Result']\n",
    "    goals_home = row['Goal_Home']\n",
    "    goals_away = row['Goal_Away']\n",
    "    \n",
    "    # Initialize the stats for the teams if they are not yet in the dictionary\n",
    "    if home_team not in team_stats:\n",
    "        team_stats[home_team] = {'Points': 0, 'GD': 0, 'GF': 0}\n",
    "    if away_team not in team_stats:\n",
    "        team_stats[away_team] = {'Points': 0, 'GD': 0, 'GF': 0}\n",
    "    \n",
    "    # Assign points and update goal statistics based on the predicted result\n",
    "    if result == 1:\n",
    "        # Home team wins\n",
    "        team_stats[home_team]['Points'] += 3\n",
    "    elif result == -1:\n",
    "        # Away team wins\n",
    "        team_stats[away_team]['Points'] += 3\n",
    "    else:\n",
    "        # Draw\n",
    "        team_stats[home_team]['Points'] += 1\n",
    "        team_stats[away_team]['Points'] += 1\n",
    "    \n",
    "    # Update goal difference (GD) and goals scored (GF)\n",
    "    team_stats[home_team]['GD'] += (goals_home - goals_away)\n",
    "    team_stats[away_team]['GD'] += (goals_away - goals_home)\n",
    "    team_stats[home_team]['GF'] += goals_home\n",
    "    team_stats[away_team]['GF'] += goals_away\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Points Table:\n",
      "               Team  Points   GD   GF\n",
      "0         Liverpool      94  114  197\n",
      "1         Tottenham      94   52  150\n",
      "2          West Ham      94   28  146\n",
      "3       Aston Villa      94  -11  117\n",
      "4           Everton      94  -25  108\n",
      "5       Southampton      94  -38  115\n",
      "6           Burnley      94  -40   83\n",
      "7           Chelsea      93   73  165\n",
      "8           Arsenal      93   43  143\n",
      "9    Crystal Palace      93  -34  101\n",
      "10        Newcastle      93  -43  103\n",
      "11  Manchester City      92  152  220\n",
      "12        Leicester      92   22  148\n",
      "13         Brighton      92  -21   91\n",
      "14   Manchester Utd      91   51  160\n",
      "15           Wolves      91  -15   90\n",
      "16            Leeds      76  -28  102\n",
      "17    Sheffield Utd      56  -46   34\n",
      "18          Norwich      56  -90   26\n",
      "19          Watford      55  -51   53\n",
      "20        Brentford      38   -8   46\n",
      "21           Fulham      38  -26   26\n",
      "22        West Brom      37  -40   33\n",
      "23      Bournemouth      18  -19   18\n",
      "\n",
      "The predicted champion is: Liverpool with 94 points, 114 goal difference, and 197 goals scored.\n"
     ]
    }
   ],
   "source": [
    "# Convert the stats dictionary into a DataFrame for better visualization\n",
    "points_table = pd.DataFrame.from_dict(team_stats, orient='index').reset_index()\n",
    "points_table.rename(columns={'index': 'Team'}, inplace=True)\n",
    "\n",
    "# Sort teams by Points, then by Goal Difference (GD), and then by Goals Scored (GF)\n",
    "points_table = points_table.sort_values(by=['Points', 'GD', 'GF'], ascending=False).reset_index(drop=True)\n",
    "\n",
    "print(\"Final Points Table:\")\n",
    "print(points_table)\n",
    "\n",
    "# Determine the champion\n",
    "champion = points_table.iloc[0]\n",
    "print(f\"\\nThe predicted champion is: {champion['Team']} with {champion['Points']} points, \"\n",
    "      f\"{champion['GD']} goal difference, and {champion['GF']} goals scored.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
